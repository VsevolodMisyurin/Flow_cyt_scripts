{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d424f8b7-0621-4c48-8bf2-b6889047e8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fcsparser import parse\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import umap\n",
    "import time\n",
    "from scipy.stats import norm\n",
    "from transliterate import translit\n",
    "import warnings\n",
    "import pickle\n",
    "from matplotlib.patches import Polygon\n",
    "from openpyxl import load_workbook\n",
    "from matplotlib.path import Path\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from scipy.interpolate import griddata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd3b4cc-f621-49d1-bf90-ef4b99bd985f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Верисия 10, стабильная\n",
    "Обновлён гейт для гранулоцитов.\n",
    "Добавил возможность настроить количество эпох при обучении классификатора.\n",
    "Больше эпох - дальше расходятся кластеры, но нужно адаптировать гейты.\n",
    "Добавил возможность переносить данные из .csv в файл заключения.\n",
    "Координаты для переноса данных тоже ставятся вручную.\n",
    "\n",
    "Версия 9, стабильна\n",
    "Исправлен гейт для мусора. Добавлены гейты для CD117, но нужно доработать\n",
    "Неудачно проведён поиск гейтов для каппа и лямбда.\n",
    "\n",
    "Версия 8 - решены проблемы с совместимостью. Необходимо добавить актуальные \n",
    "версии библиотек.С данными библиотеками популяции распознаются одинаково.\n",
    "\n",
    "Версия 7 - Готов скрипт для процессинга данных, обучения и сохранения \n",
    "классификаторов,загрузки валидационных данных, разметки популяций на юмап, \n",
    "и применения юмап к тестовым данным\n",
    "\n",
    "Необходимо:Завершить разметкуprocess_test_data функцию доработать до выдачи \n",
    "количественной экспрессии антигенов за вычетом мусораСоставить функцию для \n",
    "формирования отчёта и занесения данных в excel-бланкиПроверить, будет ли применять \n",
    "process_test_data нужный классификатор к пробирке с соответствующим названием. \n",
    "По идее, долженВынести функции для работы с тестом в отдельный файл\n",
    "Упаковать в докер или хотя бы .py-скрипт для быстрого запуска\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cae190-031a-4d65-85a5-ef53f2dbd1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = r\"C:\\Users\\vsemis\\files\\Flow_cyt_robot\" # внимание комп\n",
    "path = r\"D:\\NovoExpress Data\"  # очередной комп"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae6939c-5b8d-4e7a-9519-0c09e38a6556",
   "metadata": {},
   "source": [
    "# Используемые функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e9883d-d67e-4cd9-8a13-d9e37e2f76c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_transp_znorm_data(data):\n",
    "    \"\"\"\n",
    "    Выполняет логарифмирование всего датасета с событиями\n",
    "    После логарифмирования происходит транспонирование и z-нормализация,\n",
    "    после чего - повторное транспонирование для последующего анализа\n",
    "    Эффективность этих действий подтверждена предшествующими наблюдениями\n",
    "    \"\"\"\n",
    "    data_clipped = data.clip(lower=1)  # Клипуем данные, чтобы избежать log(0)\n",
    "    data_log = data_clipped.apply(np.log2)  # Логарифмирование\n",
    "    data_log = data_log.T  # Транспонирование\n",
    "    data_log_transp_znorm = (data_log - data_log.mean()) / data_log.std()  # z-нормализация\n",
    "    data_log_transp_znorm = data_log_transp_znorm.T  # Транспонирование\n",
    "    \n",
    "    return data_log_transp_znorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1663525f-a098-4009-9f96-af47dab987ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_percentile(df, columns=None, lower_quantile=0.0025, upper_quantile=0.99):\n",
    "    \"\"\"\n",
    "    Удаляет выбросы в датафрейме df, обрезая значения за заданными перцентилями.\n",
    "    \n",
    "    Параметры:\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Исходный датафрейм.\n",
    "    columns : list or None\n",
    "        Список столбцов, по которым ищем выбросы.\n",
    "        Если None, то берём все числовые столбцы.\n",
    "    lower_quantile : float\n",
    "        Нижняя граница (от 0 до 1), по умолчанию 0.0025 (0.25%).\n",
    "    upper_quantile : float\n",
    "        Верхняя граница (от 0 до 1), по умолчанию 0.99 (99%).\n",
    "    \n",
    "    Возвращает:\n",
    "    -----------\n",
    "    pd.DataFrame\n",
    "        Датафрейм без выбросов.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Если столбцы не указаны, берём все числовые\n",
    "    if columns is None:\n",
    "        columns = df.select_dtypes(include='number').columns.tolist()\n",
    "    \n",
    "    # Создадим копию, чтобы не менять исходный датафрейм\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    for col in columns:\n",
    "        if col not in df_clean.columns:\n",
    "            # Если в списке оказался столбец, которого нет в датафрейме\n",
    "            continue\n",
    "        \n",
    "        # Вычислим перцентили\n",
    "        low_val = df_clean[col].quantile(lower_quantile)\n",
    "        high_val = df_clean[col].quantile(upper_quantile)\n",
    "        \n",
    "        # Фильтруем выбросы\n",
    "        df_clean = df_clean[(df_clean[col] >= low_val) & (df_clean[col] <= high_val)]\n",
    "    \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdf6f22-2434-44a1-b870-816fbada98fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_arcsinh_transform_data(data, cofactor_mapping=None):  # Исправить название!!!\n",
    "    \"\"\"\n",
    "    Применяет преобразования к датафрейму.\n",
    "    \n",
    "    Для столбцов, в названии которых встречается одно из ключевых слов \n",
    "    ('CD'), применяется arcsinh-преобразование с заданным ко-фактором.\n",
    "    Для остальных столбцов выполняется клиппинг (чтобы избежать log(0)) и логарифмирование.\n",
    "    \n",
    "    Параметры:\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        Исходный датафрейм.\n",
    "    cofactor_mapping : dict, optional\n",
    "        Словарь, сопоставляющий ключевые слова и их ко-факторы.\n",
    "        По умолчанию: {'CD': 5}\n",
    "    \n",
    "    Возвращает:\n",
    "    -----------\n",
    "    pd.DataFrame\n",
    "        Датафрейм после преобразований.\n",
    "    \"\"\"\n",
    "    if cofactor_mapping is None:\n",
    "        cofactor_mapping = {'CD': 3, 'Ig': 3}  # хорошие: 5\n",
    "    \n",
    "    data_transformed = data.copy()\n",
    "    \n",
    "    for col in data_transformed.columns:\n",
    "        # Если имя столбца содержит одно из ключевых слов,\n",
    "        # применяем arcsinh-преобразование с соответствующим ко-фактором.\n",
    "        applied_arcsinh = False\n",
    "        for key, cofactor in cofactor_mapping.items():\n",
    "            if key in col:\n",
    "                data_transformed[col] = np.arcsinh(data_transformed[col] / cofactor)\n",
    "                applied_arcsinh = True\n",
    "                break  # Если нашли соответствие, дальнейшие проверки не требуются.\n",
    "                \n",
    "        # Если столбец не содержит заданных ключевых слов, применяем стандартное преобразование:\n",
    "        if not applied_arcsinh:\n",
    "            # Клиппинг для предотвращения log(0)\n",
    "            data_transformed[col] = data_transformed[col].clip(lower=1)\n",
    "            # Логарифмирование по основанию 2\n",
    "            data_transformed[col] = np.log2(data_transformed[col])\n",
    "            \n",
    "    return data_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5638abf9-03e0-42dd-aba6-88e0398534a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_umap_data_organiser(dirs, tube_list, size=10000, random_state=42):\n",
    "    \"\"\"\n",
    "    Данная функция проходит по указанным директориям, ищет файлы, соответствующие\n",
    "    именам из tube_list, загружает их, обрабатывает и сохраняет отдельные файлы \n",
    "    с названием train_data_<имя файла>.csv без расширения .fcs.\n",
    "    \"\"\"\n",
    "    for tube in tube_list:  # Проходим по каждому файлу из списка\n",
    "        combined_data = pd.DataFrame()  # Создаём пустой датафрейм\n",
    "\n",
    "        for folder in dirs:\n",
    "            print(f\"\\n📂 Обрабатываем главную папку: {folder}\")\n",
    "\n",
    "            for subfolder in os.listdir(folder):  # Проходим по подпапкам\n",
    "                subfolder_path = os.path.join(folder, subfolder)\n",
    "                if os.path.isdir(subfolder_path):\n",
    "                    print(f\"   📁 Внутренняя папка: {subfolder}\")\n",
    "\n",
    "                    for file in os.listdir(subfolder_path):\n",
    "                        if file == tube:  # Проверяем, соответствует ли имя файла текущему tube\n",
    "                            file_path = os.path.join(subfolder_path, file)\n",
    "                            print(f\"      📄 Найден файл: {file}\")\n",
    "\n",
    "                            meta, data = parse(file_path)\n",
    "                            del meta  # Не нужно, удаляем\n",
    "\n",
    "                            if len(data) > size:\n",
    "                                data = data.sample(n=size, random_state=random_state)  # Берём нужное количество строк\n",
    "                            \n",
    "                            # Удаляем ненужные столбцы\n",
    "                            data = data.drop(columns=[col for col in data.columns if col.endswith(('-A', 'Width', 'Time'))])\n",
    "\n",
    "                            # Удаляем выбросы\n",
    "                            data = remove_outliers_percentile(data, columns=['FSC-H', 'SSC-H'])\n",
    "                            \n",
    "                            # Применяем нормализацию\n",
    "                            data = log_arcsinh_transform_data(data)  # Цитометрически-ориентированный подход\n",
    "                            # data = (data - data.mean()) / data.std()  # z-нормализация\n",
    "                            \n",
    "                            # Добавляем информацию о типе образца\n",
    "                            data[\"sample_type\"] = os.path.basename(folder)\n",
    "\n",
    "                            # Добавляем колонку 'sample_name' с транслитерированным названием subfolder\n",
    "                            data[\"sample_name\"] = translit(subfolder, reversed=True)\n",
    "\n",
    "                            # Если датафрейм пуст, добавляем колонки\n",
    "                            if combined_data.empty:\n",
    "                                combined_data = pd.DataFrame(columns=data.columns)\n",
    "\n",
    "                            # Присоединяем данные\n",
    "                            combined_data = pd.concat([combined_data, data], ignore_index=True)\n",
    "\n",
    "        if not combined_data.empty:\n",
    "            # Определяем папку для сохранения (на уровень выше первой директории в dirs)\n",
    "            base_output_dir = os.path.dirname(dirs[0])\n",
    "            output_dir = os.path.join(base_output_dir, \"train_data\")  # Добавляем папку train_data\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "                print(f\"✅ Папка {output_dir} успешно создана!\")\n",
    "            else:\n",
    "                print(f\"⚠️ Папка {output_dir} уже существует.\")\n",
    "            # Формируем имя файла (убираем расширение .fcs)\n",
    "            output_filename = f\"train_data_{os.path.splitext(tube)[0]}.csv\"\n",
    "            output_path = os.path.join(output_dir, output_filename)\n",
    "\n",
    "            # Сохраняем результат\n",
    "            combined_data.to_csv(output_path, index=False)\n",
    "            print(f\"\\n✅ Данные сохранены в {output_path}\")\n",
    "        else:\n",
    "            print(f\"\\n⚠️ Файл {tube} не найден в указанных директориях.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b135f09-1207-45a0-a291-3d111759e554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для построения и обучения UMAP\n",
    "def plot_umap_from_dataframe(data, n_neighbors=20, min_dist=0.1, spread=5, metric=\"manhattan\", n_epochs=None):\n",
    "    \"\"\"\n",
    "    Выполняет UMAP для переданного DataFrame и строит UMAP-мап.\n",
    "\n",
    "    Parameters:\n",
    "    - data (pd.DataFrame): Исходный DataFrame (может содержать нечисловые данные).\n",
    "    - n_neighbors (int): Количество соседей для UMAP.\n",
    "    - min_dist (float): Минимальное расстояние между точками на карте.\n",
    "    - spread (float): Параметр, влияющий на расстояние между кластерами.\n",
    "    - metric (str): Метрика расстояния (по умолчанию \"manhattan\").\n",
    "    - n_epochs (int, optional): Количество эпох для UMAP (по умолчанию авто).\n",
    "\n",
    "    Returns:\n",
    "    - umap_df (pd.DataFrame): DataFrame с координатами UMAP.\n",
    "    - reducer (umap.UMAP): Обученный UMAP-классификатор.\n",
    "    \"\"\"\n",
    "\n",
    "    # Оставляем только числовые столбцы\n",
    "    numeric_data = data.select_dtypes(include=[np.number])\n",
    "\n",
    "    # Проверяем, что после фильтрации остались числовые данные\n",
    "    if numeric_data.empty:\n",
    "        raise ValueError(\"После удаления нечисловых колонок DataFrame оказался пустым!\")\n",
    "\n",
    "    # Подавляем предупреждения\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", message=\".*n_jobs value 1 overridden.*\")\n",
    "\n",
    "        # Инициализация и обучение UMAP\n",
    "        reducer = umap.UMAP(\n",
    "            n_components=2,\n",
    "            n_neighbors=n_neighbors,\n",
    "            random_state=42,\n",
    "            min_dist=min_dist,\n",
    "            spread=spread,\n",
    "            metric=metric,\n",
    "            n_epochs=n_epochs,  # <-- Добавляем настройку эпох\n",
    "            verbose=True  # <-- Для логов о процессе обучения\n",
    "        )\n",
    "        umap_result = reducer.fit_transform(numeric_data)\n",
    "\n",
    "    # Создаём DataFrame с результатами UMAP\n",
    "    umap_df = pd.DataFrame(umap_result, columns=['UMAP1', 'UMAP2'])\n",
    "\n",
    "    # Визуализация\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(umap_df['UMAP1'], umap_df['UMAP2'], alpha=0.6, s=10)\n",
    "    plt.title(f'UMAP Map (metric={metric}, n_epochs={n_epochs})')\n",
    "    plt.xlabel('UMAP Component 1')\n",
    "    plt.ylabel('UMAP Component 2')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return umap_df, reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3483e72c-fb16-485f-8bfe-13fabb1de81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для обучения UMAP и построения чарта, а также сохранения результатов UMAP и классификатора\n",
    "# Обращается к предшествующей функции\n",
    "def process_and_save_umap(dirs, tube, n_neighbors=25, min_dist=0.001, spread=1, metric=\"manhattan\", n_epochs=None):\n",
    "\n",
    "    \"\"\"\n",
    "    Автоматически проходит по файлам в train_data, строит UMAP и сохраняет результаты в Saved_UMAP_n_csv.\n",
    "\n",
    "    Parameters:\n",
    "    - dirs (list): Список директорий (используем для поиска Examples).\n",
    "    - tube (list): Список файлов, с которыми работали (используем для фильтрации).\n",
    "    - n_neighbors, min_dist, spread, metric: параметры для UMAP.\n",
    "    - n_epochs (int, optional): Количество эпох для UMAP (по умолчанию авто).\n",
    "    - parallel (bool): Использовать ли параллельные вычисления.\n",
    "    \"\"\"\n",
    "\n",
    "    # Определяем базовую папку \"Examples\"\n",
    "    examples_folder = os.path.dirname(dirs[0])  # Поднимаемся на уровень вверх\n",
    "    train_data_folder = os.path.join(examples_folder, \"train_data\")\n",
    "    save_folder = os.path.join(examples_folder, \"Saved_UMAP_n_csv\")\n",
    "\n",
    "    # Проверяем существование train_data\n",
    "    if not os.path.exists(train_data_folder):\n",
    "        print(f\"Ошибка: Папка {train_data_folder} не найдена!\")\n",
    "        return  \n",
    "\n",
    "    # Создаём папку для сохранения, если её нет\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "    # Получаем список CSV-файлов в train_data\n",
    "    files = [f for f in os.listdir(train_data_folder) if f.endswith(\".csv\")]\n",
    "    if not files:\n",
    "        print(\"В папке train_data нет CSV-файлов!\")\n",
    "        return\n",
    "\n",
    "    # Проходим только по файлам, которые соответствуют tube\n",
    "    for file in files:\n",
    "        # Проверяем, относится ли файл к tube (по названию без train_data_)\n",
    "        file_base_name = file.replace(\"train_data_\", \"\").replace(\".csv\", \"\")\n",
    "        if not any(tube_name.startswith(file_base_name) for tube_name in tube):\n",
    "            print(f\"Пропускаем файл {file}, так как он не в tube.\")\n",
    "            continue  \n",
    "\n",
    "        file_path = os.path.join(train_data_folder, file)\n",
    "        print(f\"\\nОбрабатываем файл: {file}\")\n",
    "\n",
    "        # Загружаем данные\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Строим UMAP\n",
    "        umap_df, reducer = plot_umap_from_dataframe(\n",
    "            df,\n",
    "            n_neighbors=n_neighbors,\n",
    "            min_dist=min_dist,\n",
    "            spread=spread,\n",
    "            metric=metric,\n",
    "            n_epochs=n_epochs,  # <-- Передаём epochs\n",
    "        )\n",
    "\n",
    "        # Создаём имена файлов для сохранения\n",
    "        base_name = os.path.splitext(file)[0]  # Убираем расширение .csv\n",
    "        df_umap_filename = f\"{base_name}_umap.csv\"\n",
    "        reducer_filename = f\"reducer_{base_name}.pkl\"\n",
    "\n",
    "        # Полные пути\n",
    "        df_umap_path = os.path.join(save_folder, df_umap_filename)\n",
    "        reducer_path = os.path.join(save_folder, reducer_filename)\n",
    "\n",
    "        # Объединяем df и umap_df\n",
    "        df_umap = pd.concat([df, umap_df], axis=1)\n",
    "\n",
    "        # Сохраняем данные\n",
    "        df_umap.to_csv(df_umap_path, index=False)  \n",
    "\n",
    "        with open(reducer_path, \"wb\") as f:\n",
    "            pickle.dump(reducer, f)\n",
    "\n",
    "        print(f\"Данные сохранены:\\n - {df_umap_path}\\n - {reducer_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6d0a9e-ac9c-4eb8-a0ff-05565c4fea75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для использования обученного UMAP\n",
    "def fit_trained_umap(reducer, new_data):\n",
    "    \"\"\"\n",
    "    Использует обученный UMAP для применения к новому набору данных.\n",
    "    \n",
    "    Parameters:\n",
    "    - reducer (umap.UMAP): Обученный UMAP-классификатор.\n",
    "    - new_data (pd.DataFrame): Новый набор данных для применения UMAP.\n",
    "    \n",
    "    Returns:\n",
    "    - umap_df (pd.DataFrame): DataFrame с результатами UMAP для нового набора данных.\n",
    "    \"\"\"\n",
    "    # Проверяем, что данные являются числовыми\n",
    "    if not all(new_data.dtypes.apply(lambda x: np.issubdtype(x, np.number))):\n",
    "        raise ValueError(\"Все столбцы в DataFrame должны быть числовыми.\")\n",
    "    \n",
    "    # Применение обученного UMAP\n",
    "    umap_result = reducer.transform(new_data)\n",
    "\n",
    "    # Создание DataFrame с результатами\n",
    "    umap_df = pd.DataFrame(umap_result, columns=[f'UMAP{i+1}' for i in range(reducer.n_components)])\n",
    "\n",
    "    # Построение UMAP-мапа\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    if reducer.n_components >= 2:\n",
    "        plt.scatter(umap_df['UMAP1'], umap_df['UMAP2'], alpha=0.6, s=10)\n",
    "        plt.title('UMAP Map (New Data)')\n",
    "        plt.xlabel('UMAP Component 1')\n",
    "        plt.ylabel('UMAP Component 2')\n",
    "    else:\n",
    "        plt.scatter(range(len(umap_df)), umap_df['UMAP1'], alpha=0.6, s=10)\n",
    "        plt.title('UMAP Map (1D Projection)')\n",
    "        plt.xlabel('Samples')\n",
    "        plt.ylabel('UMAP Component 1')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return umap_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4424e49d-d885-4e48-b6ab-117c45414947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_validation_data(dirs, tube):\n",
    "    \"\"\"\n",
    "    Обрабатывает новые данные из .fcs файлов, применяет к ним функции предварительной обработки,\n",
    "    загружает соответствующий UMAP-классификатор и строит UMAP-чарт с подписью – названием файла.\n",
    "    \n",
    "    Parameters:\n",
    "    - dirs (list of str): Список директорий, в которых находятся папки с данными (например, папка MM_subpopulations).\n",
    "    - tube (list of str): Список строк для поиска в названиях файлов (используем префикс до первой точки).\n",
    "    \n",
    "    Результат:\n",
    "    Для каждого найденного .fcs файла, удовлетворяющего условию, строится UMAP-чарт,\n",
    "    подписанный названием данного файла.\n",
    "    \"\"\"\n",
    "    # Для каждого указанного пути в списке dirs\n",
    "    for base_dir in dirs:\n",
    "        # Получаем список папок внутри base_dir\n",
    "        for folder in os.listdir(base_dir):\n",
    "            folder_path = os.path.join(base_dir, folder)\n",
    "            if not os.path.isdir(folder_path):\n",
    "                continue\n",
    "            # Сохраним название текущей папки (если нужно для логирования или дальнейшей обработки)\n",
    "            current_folder_name = folder  \n",
    "            # Проходим по всем файлам в текущей папке\n",
    "            for filename in os.listdir(folder_path):\n",
    "                if not filename.endswith(\".fcs\"):\n",
    "                    continue\n",
    "                # Для каждого элемента из tube, сравниваем префиксы (до первой точки)\n",
    "                for tube_item in tube:\n",
    "                    tube_prefix = tube_item.split('.')[0]\n",
    "                    if tube_prefix in filename:\n",
    "                        # Если найдено совпадение, формируем полный путь к файлу\n",
    "                        file_path = os.path.join(folder_path, filename)\n",
    "                        try:\n",
    "                            # Открываем .fcs файл с помощью fcsparser\n",
    "                            meta, data = parse(file_path)\n",
    "                            del meta  # метаданные не нужны, удаляем\n",
    "                        except Exception as e:\n",
    "                            print(f\"Ошибка при парсинге файла {file_path}: {e}\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Применяем функцию удаления выбросов\n",
    "                        try:\n",
    "                            data_no_outliers = remove_outliers_percentile(data, \n",
    "                                                                          columns=None, \n",
    "                                                                          lower_quantile=0.0025, \n",
    "                                                                          upper_quantile=0.99)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Ошибка при удалении выбросов для файла {filename}: {e}\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Применяем логарифмическое преобразование (arcsinh)\n",
    "                        try:\n",
    "                            data_normalized = log_arcsinh_transform_data(data_no_outliers, \n",
    "                                                                           cofactor_mapping=None)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Ошибка при нормализации данных для файла {filename}: {e}\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Формируем путь к классификаторам:\n",
    "                        # Из base_dir (например, ...\\Examples\\MM_subpopulations) поднимаемся на уровень выше (...\\Examples)\n",
    "                        # и заходим в папку Saved_UMAP_n_csv\n",
    "                        examples_dir = os.path.dirname(base_dir)\n",
    "                        classifier_dir = os.path.join(examples_dir, \"Saved_UMAP_n_csv\")\n",
    "                        \n",
    "                        # Поиск файла классификатора (.pkl), имя которого содержит tube_prefix\n",
    "                        classifier_file = None\n",
    "                        if os.path.isdir(classifier_dir):\n",
    "                            for clf in os.listdir(classifier_dir):\n",
    "                                if clf.endswith(\".pkl\") and tube_prefix in clf:\n",
    "                                    classifier_file = os.path.join(classifier_dir, clf)\n",
    "                                    break\n",
    "                        else:\n",
    "                            print(f\"Папка с классификаторами не найдена: {classifier_dir}\")\n",
    "                            continue\n",
    "                        \n",
    "                        if classifier_file is None:\n",
    "                            print(f\"Классификатор для префикса '{tube_prefix}' не найден. Пропуск файла {filename}.\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Загружаем UMAP-классификатор\n",
    "                        try:\n",
    "                            with open(classifier_file, \"rb\") as f:\n",
    "                                reducer = pickle.load(f)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Ошибка при загрузке классификатора {classifier_file}: {e}\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Применяем UMAP-классификатор к нормализованным данным\n",
    "                        try:\n",
    "                            umap_result = reducer.transform(data_normalized)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Ошибка при применении UMAP к данным файла {filename}: {e}\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Формируем DataFrame с результатами UMAP\n",
    "                        umap_df = pd.DataFrame(umap_result, \n",
    "                                               columns=[f'UMAP{i+1}' for i in range(reducer.n_components)])\n",
    "                        \n",
    "                        # Строим UMAP-чарт\n",
    "                        plt.figure(figsize=(8, 6))\n",
    "                        if reducer.n_components >= 2:\n",
    "                            plt.scatter(umap_df['UMAP1'], umap_df['UMAP2'], alpha=0.6, s=10)\n",
    "                            plt.xlabel('UMAP Component 1')\n",
    "                            plt.ylabel('UMAP Component 2')\n",
    "                        else:\n",
    "                            plt.scatter(range(len(umap_df)), umap_df['UMAP1'], alpha=0.6, s=10)\n",
    "                            plt.xlabel('Samples')\n",
    "                            plt.ylabel('UMAP Component 1')\n",
    "                        \n",
    "                        # Подписываем график названием файла\n",
    "                        plt.title(filename)\n",
    "                        plt.grid(True)\n",
    "                        plt.show()\n",
    "                        \n",
    "                        # Если файл совпал с одним из tube, то не нужно проверять оставшиеся элементы списка\n",
    "                        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaa668b-671c-4230-90f7-d697b1bfaa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Процессинг тестовых данных\n",
    "def process_test_data(dataframes, min_events=10000):\n",
    "    \"\"\"\n",
    "    Обрабатывает данные:\n",
    "    1. Обрезает до min_events строк (если возможно).\n",
    "    2. Удаляет ненужные столбцы.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframes (pd.DataFrame or list of pd.DataFrame): Один или список датафреймов.\n",
    "    - min_events (int): Минимальное количество событий для обрезки (по умолчанию 100 000).\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame или list из DataFrame (если вход был списком).\n",
    "    \"\"\"\n",
    "    single_input = isinstance(dataframes, pd.DataFrame)  # Проверяем, передан ли один DataFrame\n",
    "    if single_input:\n",
    "        dataframes = [dataframes]  # Оборачиваем в список для единообразной обработки\n",
    "\n",
    "    print(f\"⚙️  Начинаем процессинг данных...\")\n",
    "\n",
    "    # Обрезаем до нужного количества строк (если возможно)\n",
    "    processed_data = [\n",
    "        df.sample(n=min(min_events, len(df)), random_state=42).reset_index(drop=True)\n",
    "        for df in dataframes\n",
    "    ]\n",
    "    print(f\"✅ Данные обрезаны (или оставлены без изменений, если строк меньше {min_events})\")\n",
    "\n",
    "    # Удаляем столбцы, оканчивающиеся на '-A', 'Width', 'Time'\n",
    "    processed_data = [\n",
    "        df.drop(columns=[col for col in df.columns if col.endswith(('-A', 'Width', 'Time'))], errors=\"ignore\") \n",
    "        for df in processed_data\n",
    "    ]\n",
    "    print(f\"✅ Удалены столбцы: '-A', 'Width', 'Time' (если были)\")\n",
    "\n",
    "    return processed_data[0] if single_input else processed_data  # Возвращаем DataFrame, если был один вход"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80979a0-32cc-47db-b605-0e021901a11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Отображаем и колоризуем молекулы CD и проводим фильтрацию\n",
    "def cd_color_gradients(data, cd='none', sample_type='', sample_name='', transparency=0.5, subsample=0.1):\n",
    "    \"\"\"\n",
    "    Строит scatter plot UMAP1 vs UMAP2. \n",
    "    Если указан cd, то градиентно раскрашивает точки.\n",
    "    Позволяет фильтровать данные по sample_type и sample_name.\n",
    "\n",
    "    Parameters:\n",
    "    - data (pd.DataFrame): Датафрейм с колонками 'UMAP1' и 'UMAP2'.\n",
    "    - cd (str): Название колонки для градиента (по умолчанию 'none' — без окрашивания).\n",
    "    - sample_type (str): Фильтрация по типу образца (по умолчанию без фильтрации).\n",
    "    - sample_name (str): Фильтрация по имени образца (по умолчанию без фильтрации).\n",
    "    - transparency (float): Прозрачность точек (по умолчанию 0.5).\n",
    "    - subsample (float): Доля точек для отображения (от 0 до 1, по умолчанию 0.1).\n",
    "\n",
    "    Returns:\n",
    "    - None (строит график)\n",
    "    \"\"\"\n",
    "    if not (0 < subsample <= 1):\n",
    "        raise ValueError(\"subsample должен быть от 0 до 1\")\n",
    "\n",
    "    # Фильтруем по sample_type, если передан параметр\n",
    "    if sample_type and sample_type in data['sample_type'].unique():\n",
    "        data = data[data['sample_type'] == sample_type]\n",
    "\n",
    "    # Фильтруем по sample_name, если передан параметр\n",
    "    if sample_name and sample_name in data['sample_name'].unique():\n",
    "        data = data[data['sample_name'] == sample_name]\n",
    "\n",
    "    # Проверяем, осталось ли что-то после фильтрации\n",
    "    if data.empty:\n",
    "        print(\"❌ После фильтрации данных не осталось. Проверьте параметры sample_type и sample_name.\")\n",
    "        return\n",
    "\n",
    "    # Выбираем подмножество данных\n",
    "    n_samples = max(int(len(data) * subsample), 1000)  # Гарантируем минимум 1000 точек\n",
    "    sampled_data = data.sample(n=n_samples, random_state=42)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    if cd in data.columns:\n",
    "        # Создаем пользовательскую цветовую карту\n",
    "        colors = [(0, 'blue'), (0.5, 'yellow'), (1, 'red')]  # Синий -> Желтый -> Красный\n",
    "        cmap = LinearSegmentedColormap.from_list('CustomMap', colors, N=256)\n",
    "\n",
    "        plt.scatter(sampled_data['UMAP1'], sampled_data['UMAP2'], \n",
    "                    c=sampled_data[cd], cmap=cmap, alpha=transparency)\n",
    "        plt.colorbar(label=cd)  # Добавляем цветовую шкалу\n",
    "    else:\n",
    "        plt.scatter(sampled_data['UMAP1'], sampled_data['UMAP2'], \n",
    "                    color='gray', alpha=transparency)\n",
    "\n",
    "    plt.xlabel(\"UMAP1\")\n",
    "    plt.ylabel(\"UMAP2\")\n",
    "    title = f\"UMAP Projection {('with ' + cd) if cd in data.columns else ''}\"\n",
    "    if sample_type:\n",
    "        title += f\" | sample_type={sample_type}\"\n",
    "    if sample_name:\n",
    "        title += f\" | sample_name={sample_name}\"\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcd4046-ed13-4aa2-98b6-598b85f1eb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проходим по .csv-файла с сохранёнными данными UMAP, и рассматриваем, как распределена величины флуоресценции в UMAP-кластерах\n",
    "# Возможна фильтрация\n",
    "def process_umap_csv_and_plot(dirs, tube, sample_type=False, sample_name=False):\n",
    "    # Проходим по списку tube\n",
    "    for sample_name_in_tube in tube:\n",
    "        # Строим путь к .csv файлу на основе sample_name\n",
    "        # Заходим на один уровень выше папки dirs[0] и в \\Saved_UMAP_n_csv\n",
    "        saved_umap_dir = os.path.join(os.path.dirname(dirs[0]), 'Saved_UMAP_n_csv')\n",
    "\n",
    "        # Ищем .csv файл с нужным именем и содержащим слово \"umap\" в названии\n",
    "        csv_file = None\n",
    "        for file in os.listdir(saved_umap_dir):\n",
    "            # Проверка на наличие префикса и \"umap\" в названии файла\n",
    "            if sample_name_in_tube.split('.')[0] in file and 'umap' in file.lower() and file.endswith('.csv'):\n",
    "                csv_file = os.path.join(saved_umap_dir, file)\n",
    "                break\n",
    "\n",
    "        if csv_file is None:\n",
    "            print(f\"Не найден .csv файл для {sample_name_in_tube} в папке {saved_umap_dir}\")\n",
    "            continue\n",
    "\n",
    "        # Загружаем данные из найденного файла\n",
    "        data = pd.read_csv(csv_file)\n",
    "\n",
    "        # Сохраняем список всех столбцов, кроме столбцов для типа образца и 'UMAP1' и 'UMAP2'\n",
    "        columns_to_plot = [col for col in data.columns if col not in ['sample_type', 'sample_name', 'UMAP1', 'UMAP2']]\n",
    "\n",
    "        # Если sample_type=True, то перебираем уникальные значения в столбце sample_type\n",
    "        if sample_type:\n",
    "            unique_sample_types = data['sample_type'].unique()\n",
    "            for sample in unique_sample_types:\n",
    "                print(f\"Построение графиков для sample_type: {sample}\")\n",
    "                # Фильтруем данные по текущему значению sample_type\n",
    "                filtered_data = data[data['sample_type'] == sample]\n",
    "\n",
    "                # Для каждого столбца строим график\n",
    "                for col in columns_to_plot:\n",
    "                    cd_color_gradients(filtered_data, cd=col, sample_type=sample, sample_name=sample_name_in_tube)\n",
    "        else:\n",
    "            # Если sample_type=False, строим графики для всех данных без фильтрации\n",
    "            for col in columns_to_plot:\n",
    "                cd_color_gradients(data, cd=col, sample_type=sample_type, sample_name=sample_name_in_tube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380336de-29b0-4ece-ba61-f8f2904ce1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для настройки полигонов\n",
    "def umap_populations_plot(path, dirs, tube, polygons, subsample=0.1, sample_type=None, sample_name=None):\n",
    "    # Извлекаем фрагмент из tube до первой точки\n",
    "    tube_prefix = tube.split('.')[0]\n",
    "    \n",
    "    # 1. Строим путь к папке Saved_UMAP_n_csv через первый элемент из dirs\n",
    "    saved_path = os.path.join(os.path.dirname(dirs[0]), \"Saved_UMAP_n_csv\")\n",
    "    \n",
    "    # 2. Ищем нужный CSV-файл\n",
    "    matching_files = [f for f in os.listdir(saved_path) if tube_prefix in f and \"umap\" in f.lower() and f.endswith(\".csv\")]\n",
    "\n",
    "    if not matching_files:\n",
    "        raise FileNotFoundError(f\"Файл с фрагментом '{tube_prefix}' не найден в {saved_path}\")\n",
    "    \n",
    "    csv_file = os.path.join(saved_path, matching_files[0])\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # 3. Фильтрация данных, если заданы sample_type и sample_name\n",
    "    if sample_type is not None:\n",
    "        df = df[df[\"sample_type\"] == sample_type]\n",
    "    if sample_name is not None:\n",
    "        df = df[df[\"sample_name\"] == sample_name]\n",
    "\n",
    "    # Если после фильтрации данных нет, выбрасываем предупреждение\n",
    "    if df.empty:\n",
    "        print(f\"⚠️ После фильтрации данных ({sample_type=}, {sample_name=}) не осталось! Возвращаю пустой DataFrame.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # 4. Выбираем случайную подвыборку\n",
    "    df_sampled = df.sample(frac=subsample, random_state=42) if subsample < 1.0 else df\n",
    "    \n",
    "    # 5. Строим график\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.scatter(df_sampled[\"UMAP1\"], df_sampled[\"UMAP2\"], s=5, alpha=0.5, label=\"Data\")\n",
    "    \n",
    "    # 6. Добавляем полигоны и метки\n",
    "    res = df.copy()\n",
    "    legend_entries = set()  # Храним добавленные (имя, цвет)\n",
    "\n",
    "    for poly in polygons:\n",
    "        name, color, *coords = poly\n",
    "        polygon = np.array(coords)\n",
    "        poly_patch = Polygon(polygon, edgecolor=color, facecolor=color, alpha=0.3)\n",
    "\n",
    "        # Добавляем в легенду только если такой (имя, цвет) ещё не был добавлен\n",
    "        if (name, color) not in legend_entries:\n",
    "            legend_entries.add((name, color))\n",
    "            poly_patch.set_label(name)  # Добавляем имя только один раз\n",
    "\n",
    "        ax.add_patch(poly_patch)\n",
    "\n",
    "        # Проверяем, какие точки попадают внутрь полигона\n",
    "        path = Path(polygon)\n",
    "        inside = path.contains_points(df[[\"UMAP1\", \"UMAP2\"]].values)\n",
    "        if name in res:\n",
    "            res[name] |= inside.astype(np.int8)  # Логическое ИЛИ\n",
    "        else:\n",
    "            res[name] = inside.astype(np.int8)  # Добавляем как int8\n",
    "\n",
    "    ax.legend()\n",
    "    plt.xlabel(\"UMAP1\")\n",
    "    plt.ylabel(\"UMAP2\")\n",
    "    plt.title(f\"UMAP Populations: {tube}\")\n",
    "    plt.show()\n",
    "\n",
    "    # Сохранение координат полигонов с индексом n\n",
    "    polygon_data = []\n",
    "    polygon_counts = {}  # Считаем количество одинаковых названий\n",
    "    \n",
    "    for i, poly in enumerate(polygons):\n",
    "        name, color, *coords = poly\n",
    "        if name not in polygon_counts:\n",
    "            polygon_counts[name] = 1\n",
    "        else:\n",
    "            polygon_counts[name] += 1\n",
    "    \n",
    "        poly_index = polygon_counts[name]  # Индексируем повторяющиеся названия\n",
    "    \n",
    "        for x, y in coords:\n",
    "            polygon_data.append([name, color, x, y, poly_index])\n",
    "    \n",
    "    polygon_df = pd.DataFrame(polygon_data, columns=[\"name\", \"color\", \"x\", \"y\", \"n\"])\n",
    "    polygon_df.to_csv(os.path.join(saved_path, f\"polygons_{tube_prefix}.csv\"), index=False)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa48756-66d6-4b6b-8bdc-d0ec82372bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_test_data(path, tube, pdf=False, fcs_del=True):\n",
    "    reports_dir = os.path.join(path, \"Reports\")\n",
    "    os.makedirs(reports_dir, exist_ok=True)  # Создаём папку Reports в path\n",
    "\n",
    "    saved_path = os.path.join(path, \"Examples\", \"Saved_UMAP_n_csv\")  # Исправленный путь к классификатору\n",
    "\n",
    "    # Обходим все папки в корневом каталоге, исключая служебные папки\n",
    "    for folder_name in sorted(os.listdir(path)):\n",
    "        if folder_name in [\"Reports\", \"Examples\"]:\n",
    "            continue\n",
    "\n",
    "        folder_path = os.path.join(path, folder_name)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "\n",
    "        # Собираем файлы по порядку tube\n",
    "        ordered_files = []\n",
    "        for tube_item in tube:\n",
    "            tube_prefix = tube_item.split('.')[0]\n",
    "            files_for_prefix = [f for f in os.listdir(folder_path) if tube_prefix in f and f.endswith(\".fcs\")]\n",
    "            ordered_files.extend(files_for_prefix)\n",
    "\n",
    "        if not ordered_files:\n",
    "            continue  # Пропускаем папку, если нет подходящих .fcs файлов\n",
    "\n",
    "        pdf_pages = None\n",
    "        if pdf:\n",
    "            pdf_path = os.path.join(reports_dir, f\"Report of {folder_name}.pdf\")\n",
    "            pdf_pages = PdfPages(pdf_path)\n",
    "            csv_path = os.path.join(reports_dir, f\"Report_of_{folder_name}.csv\")\n",
    "\n",
    "        print(f\"\\nОбрабатываю папку: {folder_name}\")\n",
    "        for filename in ordered_files:\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "            # Определяем tube_prefix\n",
    "            tube_prefix = None\n",
    "            for tube_item in tube:\n",
    "                prefix = tube_item.split('.')[0]\n",
    "                if prefix in filename:\n",
    "                    tube_prefix = prefix\n",
    "                    break\n",
    "            if tube_prefix is None:\n",
    "                print(f\"Префикс для файла {filename} не найден.\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                meta, data = parse(file_path)\n",
    "                del meta\n",
    "                data_no_outliers = remove_outliers_percentile(data)\n",
    "                data_normalized = log_arcsinh_transform_data(data_no_outliers)\n",
    "            except Exception as e:\n",
    "                print(f\"Ошибка при обработке {filename}: {e}\")\n",
    "                continue\n",
    "\n",
    "            classifier_file = None\n",
    "            if os.path.isdir(saved_path):\n",
    "                for clf in os.listdir(saved_path):\n",
    "                    if clf.endswith(\".pkl\") and tube_prefix in clf:\n",
    "                        classifier_file = os.path.join(saved_path, clf)\n",
    "                        break\n",
    "            if classifier_file is None:\n",
    "                print(f\"Классификатор для {tube_prefix} не найден в {saved_path}.\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with open(classifier_file, \"rb\") as f:\n",
    "                    reducer = pickle.load(f)\n",
    "                umap_result = reducer.transform(data_normalized)\n",
    "            except Exception as e:\n",
    "                print(f\"Ошибка при применении UMAP {filename}: {e}\")\n",
    "                continue\n",
    "\n",
    "            umap_df = pd.DataFrame(umap_result, columns=[f'UMAP{i+1}' for i in range(reducer.n_components)])\n",
    "\n",
    "            polygon_file = os.path.join(saved_path, f\"polygons_{tube_prefix}.csv\")\n",
    "            if not os.path.exists(polygon_file):\n",
    "                print(f\"Файл с полигонами {polygon_file} не найден.\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                polygons_df = pd.read_csv(polygon_file)\n",
    "            except Exception as e:\n",
    "                print(f\"Ошибка при загрузке полигонов {polygon_file}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # ---- Визуализация UMAP ----\n",
    "            fig, ax = plt.subplots(figsize=(10, 8))\n",
    "            ax.scatter(umap_df[\"UMAP1\"], umap_df[\"UMAP2\"], s=5, alpha=0.5, label=\"Data\")\n",
    "\n",
    "            umap_df_result = umap_df.copy()\n",
    "            polygon_groups = {}\n",
    "            added_legend_items = set()\n",
    "\n",
    "            for (name, n) in polygons_df[[\"name\", \"n\"]].drop_duplicates().itertuples(index=False):\n",
    "                poly_data = polygons_df[(polygons_df[\"name\"] == name) & (polygons_df[\"n\"] == n)]\n",
    "                polygon = np.array(list(zip(poly_data[\"x\"], poly_data[\"y\"])))\n",
    "                poly_color = poly_data[\"color\"].iloc[0]\n",
    "                legend_key = (name, poly_color)\n",
    "                poly_patch = Polygon(polygon, edgecolor=poly_color, facecolor=\"none\", lw=2)\n",
    "                ax.add_patch(poly_patch)\n",
    "                if legend_key not in added_legend_items:\n",
    "                    ax.plot([], [], color=poly_color, label=name)\n",
    "                    added_legend_items.add(legend_key)\n",
    "                path_obj = Path(polygon)\n",
    "                inside = path_obj.contains_points(umap_df[[\"UMAP1\", \"UMAP2\"]].values).astype(np.int32)\n",
    "                if name in polygon_groups:\n",
    "                    polygon_groups[name] |= inside\n",
    "                else:\n",
    "                    polygon_groups[name] = inside\n",
    "\n",
    "            for name, inside_mask in polygon_groups.items():\n",
    "                umap_df_result[name] = inside_mask\n",
    "\n",
    "            ax.legend()\n",
    "            plt.xlabel(\"UMAP1\")\n",
    "            plt.ylabel(\"UMAP2\")\n",
    "            plt.title(f\"Тестовые данные: {filename}\")\n",
    "\n",
    "            # ---- Подготовка статистики ----\n",
    "            stats = umap_df_result.describe().T.iloc[2:, :2]\n",
    "            if \"Trash\" in umap_df_result.columns:\n",
    "                total_cells = len(umap_df_result)\n",
    "                trash_count = (umap_df_result[\"Trash\"] == 1).sum()\n",
    "                granulocytes_count = (umap_df_result[\"Granulocytes\"] == 1).sum() if \"Granulocytes\" in umap_df_result.columns else 0\n",
    "                mononuclears = total_cells - trash_count - granulocytes_count\n",
    "                stats.loc[\"Mononuclears\", [\"count\", \"mean\"]] = [mononuclears, mononuclears / total_cells * 100]\n",
    "                if \"Granulocytes\" in umap_df_result.columns:\n",
    "                    stats.loc[\"Granulocytes\", [\"count\", \"mean\"]] = [granulocytes_count, granulocytes_count / (total_cells - trash_count) * 100]\n",
    "                for pop in polygon_groups.keys():\n",
    "                    if pop in umap_df_result.columns and pop not in [\"Trash\", \"Granulocytes\"]:\n",
    "                        count = umap_df_result[pop].sum()\n",
    "                        stats.loc[pop, [\"count\", \"mean\"]] = [count, count / mononuclears * 100]\n",
    "\n",
    "            stats.columns = ['Абс. количество', 'Содержание, %']\n",
    "            stats['Абс. количество'] = stats['Абс. количество'].astype(np.int32)\n",
    "            stats['Содержание, %'] = stats['Содержание, %'].astype(np.float32).round(4)\n",
    "            stats = stats.drop(\"Trash\", errors=\"ignore\")\n",
    "\n",
    "            print(f\"\\nСтатистика для пробирки {filename} (пациент: {folder_name}):\")\n",
    "            plt.show()\n",
    "            display(stats)\n",
    "            plt.close(fig)\n",
    "\n",
    "            if pdf and pdf_pages:\n",
    "                pdf_pages.savefig(fig)\n",
    "                plt.close(fig)\n",
    "                fig_table, ax_table = plt.subplots(figsize=(8, 4))\n",
    "                ax_table.axis('tight')\n",
    "                ax_table.axis('off')\n",
    "                table_data = stats.reset_index().values.tolist()\n",
    "                table_data.insert(0, [\"Группа\", \"Абс. количество\", \"Содержание, %\"])\n",
    "                ax_table.table(cellText=table_data, loc=\"center\", cellLoc=\"center\")\n",
    "                plt.title(f\"Статистика для образца {folder_name}.\")\n",
    "                pdf_pages.savefig(fig_table)\n",
    "                plt.close(fig_table)\n",
    "            if pdf:\n",
    "                stats.to_csv(csv_path, mode='a', header=not os.path.exists(csv_path))\n",
    "                print(f\"Отчёт сохранён в {csv_path}\")\n",
    "\n",
    "        if pdf and pdf_pages:\n",
    "            pdf_pages.close()\n",
    "            print(f\"Отчёт сохранён в {pdf_path}\")\n",
    "\n",
    "        if fcs_del:\n",
    "            shutil.rmtree(folder_path)\n",
    "            print(f\"Удалена папка: {folder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f6cac4-ff5c-4e9e-9057-bee81a8b3664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для формирования отчётов\n",
    "def make_report(path, blanks, coordinates, clean=True):\n",
    "    reports_folder = os.path.join(path, \"Reports\")\n",
    "    \n",
    "    # Получаем список CSV-файлов, убирая \"Report_of_\" из начала\n",
    "    report_files = [f for f in os.listdir(reports_folder) if f.endswith(\".csv\")]\n",
    "    report_names = [f.replace(\"Report_of_\", \"\").replace(\".csv\", \"\") for f in report_files]\n",
    "\n",
    "    # Загружаем файл координат, не превращая первый столбец в индекс\n",
    "    coordinates_file = os.path.join(coordinates, \"Coordinates.xlsx\")\n",
    "    df_coords = pd.read_excel(coordinates_file, index_col=None)\n",
    "\n",
    "    for report_name in report_names:\n",
    "        csv_path = os.path.join(reports_folder, f\"Report_of_{report_name}.csv\")\n",
    "\n",
    "        # Ищем подходящий XLSX-файл в папке blanks (начинается с report_name, затем любые символы)\n",
    "        matching_files = [f for f in os.listdir(blanks) if re.match(fr\"^{re.escape(report_name)}.*\\.xlsx$\", f)]\n",
    "        if not matching_files:\n",
    "            continue\n",
    "\n",
    "        # Берём первый найденный файл (если их несколько)\n",
    "        xlsx_path = os.path.join(blanks, matching_files[0])\n",
    "        if not os.path.exists(csv_path):\n",
    "            continue\n",
    "\n",
    "        # Загружаем CSV-файл с учетом хедера\n",
    "        df_report = pd.read_csv(csv_path, header=0)\n",
    "        \n",
    "        # Получаем значение для фильтрации из первого столбца CSV\n",
    "        filter_value = df_report.iloc[0, 0]\n",
    "        \n",
    "        # Фильтруем координаты по столбцу \"Col0str1_in_report\"\n",
    "        df_filtered = df_coords[df_coords[\"Col0str1_in_report\"] == filter_value]\n",
    "        if df_filtered.empty:\n",
    "            continue\n",
    "\n",
    "        # Открываем XLSX и сохраняем размеры ячеек\n",
    "        wb = load_workbook(xlsx_path)\n",
    "        ws = wb.active\n",
    "        column_widths = {col: ws.column_dimensions[col].width for col in ws.column_dimensions}\n",
    "        row_heights = {row: ws.row_dimensions[row].height for row in ws.row_dimensions}\n",
    "\n",
    "        # Перенос данных с проверкой на отсутствие данных в Data_in_report_position\n",
    "        for _, row in df_filtered.iterrows():\n",
    "            blank_cell = str(row[\"Data_in_blank_position\"]).strip()\n",
    "            if pd.isna(row[\"Data_in_report_position\"]):\n",
    "                ws[blank_cell] = 0\n",
    "                ws[blank_cell].number_format = '0.00'\n",
    "            else:\n",
    "                report_index = int(row[\"Data_in_report_position\"])\n",
    "                if report_index < len(df_report):\n",
    "                    # Берем данные из столбца \"Mean, %\" (предполагается, что это третий столбец)\n",
    "                    value_to_copy = df_report.iloc[report_index, 2]\n",
    "                    ws[blank_cell] = value_to_copy\n",
    "                    ws[blank_cell].number_format = '0.00'\n",
    "                    \n",
    "        # Восстанавливаем размеры ячеек\n",
    "        for col, width in column_widths.items():\n",
    "            ws.column_dimensions[col].width = width\n",
    "        for row, height in row_heights.items():\n",
    "            ws.row_dimensions[row].height = height\n",
    "\n",
    "        # Сохраняем XLSX\n",
    "        wb.save(xlsx_path)\n",
    "        wb.close()\n",
    "\n",
    "        # Удаляем CSV, если clean=True\n",
    "        if clean:\n",
    "            os.remove(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6771ca81-441c-402a-bddb-9555b96b9573",
   "metadata": {},
   "source": [
    "# Процессинг данных, обучение и сохранение UMAP-классификаторов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fde981a-5a5a-44b8-a8e0-0f2594e7d110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Укажем адреса папок с сырыми данными, пробирку и вызовем функцию для организации данных перед обучением UMAP\n",
    "dirs = [path + r\"\\Examples\\Normal_MM\", \n",
    "        path + r\"\\Examples\\MM\"]\n",
    "tube = [r\"10_56_45_19_138_38.fcs\", r\"λ_κ_45_117_138_38.fcs\", r\"xx_79a_45_Ki67_3_xx.fcs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8634ec29-d4e8-4667-8709-76484ed10bf5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Организация данных для обучения UMAP\n",
    "train_umap_data_organiser(dirs, tube, size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a6fdfe-2514-404b-89e5-598dde8af453",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Автоматически обучаем классификаторы UMAP по полученным данным, и сохраняем\n",
    "process_and_save_umap(dirs, \n",
    "                      tube, \n",
    "                      n_neighbors=25, \n",
    "                      min_dist=0.001, \n",
    "                      metric=\"manhattan\", \n",
    "                      n_epochs=None,            # n_epochs=None по умолчанию. Для более 10к точек это 200\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fd1ffe-d507-481f-a1c8-5c18bac3a95c",
   "metadata": {},
   "source": [
    "# Анализ результатов работы UMAP-классификаторов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7a8cb9-6dcb-4992-bcb6-8b7e4a4f0b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f97b9d-710d-43c9-a069-f997a1cbdd4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Вызов функции с передачей нужных директорий и списка tube\n",
    "process_umap_csv_and_plot(dirs, tube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccea1ff-4a99-4985-92d4-432a3d7f12df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Вызов функции с передачей нужных директорий и списка tube, и с фильтрацией по типу образца\n",
    "process_umap_csv_and_plot(dirs, tube, sample_type=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b15ffc-2ddf-4459-aae3-1ae064cf8a31",
   "metadata": {},
   "source": [
    "# Визуализация отдельных популяций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3f9e90-664c-4d8c-b455-15f4eaa3ea90",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = [path + r\"\\Examples\\MM_subpopulations\"]  # Валидационные данные находятся в отдельной папке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecccbf17-9e55-4877-855d-d55cc9f3b849",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "process_validation_data(dirs, tube=[r\"10_56_45_19_138_38.fcs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88971231-24c7-4c32-909e-0f81b44f9769",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "process_validation_data(dirs, tube=[r\"λ_κ_45_117_138_38.fcs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d032755-2389-4443-ac6e-b62532eac007",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "process_validation_data(dirs, tube=[r\"xx_79a_45_Ki67_3.fcs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974c918e-9c6b-461c-a1d9-3ce80dd54dc1",
   "metadata": {},
   "source": [
    "# Анализ реального случая"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9ba7dc-edb0-4e33-aa95-c2f4d22d382d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = [path + r\"\\Examples\\Test_MM\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42adebd0-abba-4f92-b92f-301b07faf75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "process_validation_data(dirs, tube=[r\"10_56_45_19_138_38.fcs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31285763-e843-4ae8-b9ea-aebe8a5f7cc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "process_validation_data(dirs, tube=[r\"λ_κ_45_117_138_38.fcs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169db836-86ce-4254-ad47-d8cd3ec22710",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "process_validation_data(dirs, tube=[r\"xx_79a_45_Ki67_3.fcs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93b75dd-c197-4ad9-8552-78209bb449ee",
   "metadata": {},
   "source": [
    "# Эксперименты по разметке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095c2f44-3120-4f0a-92a6-a8feffeb60ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = [path + r\"\\Examples\\Normal_MM\", \n",
    "        path + r\"\\Examples\\MM\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b087a2af-4bbb-437b-97c9-62e7b5a669fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Настраиваем полигоны\n",
    "res = umap_populations_plot(\n",
    "    path=path, dirs=dirs,\n",
    "    tube=\"10_56_45_19_138_38.fcs\",\n",
    "    polygons=[\n",
    "        [\"CD138+CD56+CD38+\", \"gray\", (2, 15), (4, 15), (5, 13), (6.5, 11), (5.5, 9.3), (4, 9)],\n",
    "        [\"CD138+CD38+\", \"red\", (2, 15), (4, 15), (5, 13), (6.5, 11), (5.5, 9.3), (4, 9)],\n",
    "        [\"Plasma cells\", \"orange\", (7.5, 8), (8.5, 9), (9, 8.6), (9.5, 8), (10, 6), (8.5, 6)],\n",
    "        [\"Plasma cells\", \"orange\", (4, 9), (6.8, 9.5), (5, 6)],\n",
    "        [\"CD138+\", \"red\", (2, 15), (4, 15), (5, 13), (6.5, 11), (5.5, 9.3), (4, 9)],\n",
    "        [\"CD138+\", \"red\", (7.5, 8), (8.5, 9), (9, 8.6), (9.5, 8), (10, 6), (8.5, 6)],\n",
    "        [\"CD38+\", \"brown\", (7.5, 8), (8.5, 9), (9, 8.6), (9.5, 8), (10, 6), (8.5, 6)],\n",
    "        [\"CD138+CD38+\", \"red\", (7.5, 8), (8.5, 9), (9, 8.6), (9.5, 8), (10, 6), (8.5, 6)],\n",
    "        [\"CD38+\", \"brown\", (2, 15), (4, 15), (5, 13), (6.5, 11), (5.5, 9.3), (4, 9)],\n",
    "        [\"CD10+\", \"green\", (6.5, 11.5), (7.5, 13), (8, 12), (7.5, 11)],\n",
    "        [\"CD56+\", \"yellow\", (2, 15), (4, 15), (5, 13), (6.5, 11), (5.5, 9.3), (4, 9)],\n",
    "        [\"Lymphocytes\", \"teal\", (5, -2), (9, 0), (12, -5)],\n",
    "        [\"CD56+\", \"yellow\", (6, -2), (8.5,-1), (9, -4)],\n",
    "        [\"CD19+\", \"blue\", (8.5, -1), (9, -0.5), (9.5, -1.5), (8.65, -1.75)],\n",
    "        [\"CD19+\", \"blue\", (4, 9), (6.8, 9.5), (5, 6)],\n",
    "        [\"CD38+\", \"brown\", (7, -2.3), (7, -1.3), (8.5, -1.3), (8.5, -2.3)],\n",
    "        [\"Monocytes\", \"blue\", (10, 5), (10.5, 7.5), (11, 7.5), (11, 5), (11.5, 4), (11, 3), (10.2, 4.2)],\n",
    "        [\"Granulocytes\", \"orange\", (12, 9), (18, 9), (18, 1.5), (16.5, 1.5), (15.5, 5), (13, 7.3), (11.5, 8)],\n",
    "        [\"Granulocytes\", \"yellow\", (11.5, 8), (13, 7.3), (15.5, 5), (16.5, 1.5), (12.5, 2), (11.5, 4), (11, 5)],\n",
    "        [\"Trash\", \"black\", (-7, 15), (0, 15), (3, 7), (5, 6), (7, 10), (8, 12), (9, 16), \n",
    "        (12.5, 16), (10, 12.5), (8, 10), (7.5, 8), (8, 7), (9.7, 3), (5, -2), (5, -8), (-7, -7)]\n",
    "    ],\n",
    "    subsample=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab3d0bd-57f4-4128-9cf7-3450eb63278b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c55dc19-09ce-4a4d-ba27-b2b053e0165c",
   "metadata": {},
   "outputs": [],
   "source": [
    "res['Trash'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005b4f8e-5dde-4311-9ace-205cbd22074c",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37712bb-2098-4549-9b45-a19f986e96d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "types = res.sample_type.unique().tolist()\n",
    "samples = res.sample_name.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be965723-7218-4a79-b54f-e5ff9fd826a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in types:\n",
    "    for j in samples:\n",
    "        res = umap_populations_plot(\n",
    "            path=path, dirs=dirs,\n",
    "            tube=\"10_56_45_19_138_38.fcs\",\n",
    "            sample_type=i,\n",
    "            sample_name=j,\n",
    "            polygons=[\n",
    "                [\"CD138+CD56+CD38+\", \"gray\", (2, 15), (4, 15), (5, 13), (6.5, 11), (5.5, 9.3), (4, 9)],\n",
    "                [\"CD138+CD38+\", \"red\", (2, 15), (4, 15), (5, 13), (6.5, 11), (5.5, 9.3), (4, 9)],\n",
    "                [\"Plasma cells\", \"orange\", (7.5, 8), (8.5, 9), (9, 8.6), (9.5, 8), (9.7, 7), (10, 6), (8.5, 6)],\n",
    "                [\"Plasma cells\", \"orange\", (4, 9), (6.8, 9.5), (5, 6)],\n",
    "                [\"CD138+\", \"red\", (2, 15), (4, 15), (5, 13), (6.5, 11), (5.5, 9.3), (4, 9)],\n",
    "                [\"CD138+\", \"red\", (7.5, 8), (8.5, 9), (9, 8.6), (9.5, 8), (10, 6), (8.5, 6)],\n",
    "                [\"CD38+\", \"brown\", (7.5, 8), (8.5, 9), (9, 8.6), (9.5, 8), (10, 6), (8.5, 6)],\n",
    "                [\"CD138+CD38+\", \"red\", (7.5, 8), (8.5, 9), (9, 8.6), (9.5, 8), (10, 6), (8.5, 6)],\n",
    "                [\"CD38+\", \"brown\", (2, 15), (4, 15), (5, 13), (6.5, 11), (5.5, 9.3), (4, 9)],\n",
    "                [\"CD10+\", \"green\", (6.5, 11.5), (7.5, 13), (8, 12), (7.5, 11)],\n",
    "                [\"CD56+\", \"yellow\", (2, 15), (4, 15), (5, 13), (6.5, 11), (5.5, 9.3), (4, 9)],\n",
    "                [\"Lymphocytes\", \"teal\", (5, -2), (9, 0), (12, -5)],\n",
    "                [\"CD56+\", \"yellow\", (6, -2), (8.5,-1), (9, -4)],\n",
    "                [\"CD19+\", \"blue\", (8.5, -1), (9, -0.5), (9.5, -1.5), (8.65, -1.75)],\n",
    "                [\"CD19+\", \"blue\", (4, 9), (6.8, 9.5), (5, 6)],\n",
    "                [\"CD38+\", \"brown\", (7, -2.3), (7, -1.3), (8.5, -1.3), (8.5, -2.3)],\n",
    "                [\"Monocytes\", \"blue\", (10, 5), (10.5, 7.5), (11, 7.5), (11, 5), (11.5, 4), (11, 3), (10.2, 4.2)],\n",
    "                [\"Granulocytes\", \"orange\", (12, 9), (18, 9), (18, 1.5), (16.5, 1.5), (15.5, 5), (13, 7.3), (11.5, 8)],\n",
    "                [\"Granulocytes\", \"yellow\", (11.5, 8), (13, 7.3), (15.5, 5), (16.5, 1.5), (12.5, 2), (11.5, 4), (11, 5)],\n",
    "                [\"Trash\", \"black\", (-7, 15), (0, 15), (3, 7), (5, 6), (7, 10), (8, 12), (9, 16), \n",
    "                (12.5, 16), (10, 12.5), (8, 10), (7.5, 8), (8, 7), (9.7, 3), (5, -2), (5, -8), (-7, -7)]\n",
    "            ],\n",
    "            subsample=1\n",
    "        )\n",
    "\n",
    "        print(i)\n",
    "        print(j)\n",
    "        if res is not None and not res.empty:\n",
    "            print(res.describe().T.tail(9).iloc[:, :2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9192a0-ecbc-44e4-afa7-5f1bd8976933",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Настраиваем полигоны\n",
    "res = umap_populations_plot(\n",
    "    path=path, dirs=dirs,\n",
    "    tube=\"λ_κ_45_117_138_38.fcs\",\n",
    "    polygons=[\n",
    "        [\"CD138+CD38+\", \"gray\", (5, 12), (7, 10), (6.8, 8.3), (6, 7.7), (7.3, 5), (7.5, 4.5), (2, 7)],\n",
    "        [\"CD117+\", \"violet\", (7.3, 10.9), (8.8, 9.6), (8.8, 7.5), (7, 10),\n",
    "        (6.8, 8.3), (6, 7.7), (5.8, 9), (6, 9.8), (7, 10)],\n",
    "        [\"kappa+\", \"pink\", (5, 12), (6, 10), (6, 9), (6, 7.8), (5, 8.25), (4, 10)],\n",
    "        [\"lambda+\", \"green\", (4, 10.2), (5, 8.25), (6, 7.8), (7.5, 4.5), (2, 7)],\n",
    "        [\"Plasma cells\", \"orange\", (5, 12), (7, 10), (6.8, 8.3), (6, 7.7), (7.5, 4.5), (2, 7)],\n",
    "        [\"Lymphocytes\", \"teal\", (11, -2), (16, -1), (10, -8)],\n",
    "        [\"Monocytes\", \"blue\", (6.8, 8.3), (8, 8), (9, 7), (9.3, 5), (7.8, 6), (7, 6.7), (6.3, 6.7), (6, 7.7)],\n",
    "        [\"Granulocytes\", \"orange\", (10, 10), (13.5, 12.5), (18, 8), (18, 1.5), (15.5, 3.5), (12.5, 2.5), (10, 6)],\n",
    "        [\"Trash\", \"black\", (7.5, 4.8), (8, 5.3), (9, 5), (9, -10), (-5, -10), (-5, 15), (6, 15),\n",
    "        (7.5, 12), (7, 10), (5, 12), (2, 7), (5, 2.5)]\n",
    "    ],\n",
    "    subsample=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e06694f-5206-47c0-aa80-b5b086ed9d46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in types:\n",
    "    for j in samples:\n",
    "        res = umap_populations_plot(\n",
    "            path=path, dirs=dirs,\n",
    "            tube=\"λ_κ_45_117_138_38.fcs\",\n",
    "            sample_type=i,\n",
    "            sample_name=j,\n",
    "            polygons=[\n",
    "                [\"CD138+CD38+\", \"gray\", (5, 12), (7, 10), (6.8, 8.3), (6, 7.7), (7.3, 5), (7.5, 4.5), (2, 7)],\n",
    "                [\"CD117+\", \"violet\", (7.3, 10.9), (8.8, 9.6), (8.8, 7.5), (7, 10),\n",
    "                (6.8, 8.3), (6, 7.7), (5.8, 9), (6, 9.8), (7, 10)],\n",
    "                [\"kappa+\", \"pink\", (5, 12), (6, 10), (6, 9), (6, 7.8), (5, 8.25), (4, 10)],\n",
    "                [\"lambda+\", \"green\", (4, 10.2), (5, 8.25), (6, 7.8), (7.5, 4.5), (2, 7)],\n",
    "                [\"Plasma cells\", \"orange\", (5, 12), (7, 10), (6.8, 8.3), (6, 7.7), (7.5, 4.5), (2, 7)],\n",
    "                [\"Lymphocytes\", \"teal\", (11, -2), (16, -1), (10, -8)],\n",
    "                [\"Monocytes\", \"blue\", (6.8, 8.3), (8, 8), (9, 7), (9.3, 5), (7.8, 6), (7, 6.7), (6.3, 6.7), (6, 7.7)],\n",
    "                [\"Granulocytes\", \"orange\", (10, 10), (13.5, 12.5), (18, 8), (18, 1.5), (15.5, 3.5), (12.5, 2.5), (10, 6)],\n",
    "                [\"Trash\", \"black\", (7.5, 4.8), (8, 5.3), (9, 5), (9, -10), (-5, -10), (-5, 15), (6, 15),\n",
    "                (7.5, 12), (7, 10), (5, 12), (2, 7), (5, 2.5)]\n",
    "            ],\n",
    "            subsample=1\n",
    "        )\n",
    "\n",
    "        print(i)\n",
    "        print(j)\n",
    "        if res is not None and not res.empty:\n",
    "            print(res.describe().T.tail(9).iloc[:, :2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29989501-0e94-4b77-ab4a-b3856829210e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Настраиваем полигоны\n",
    "res = umap_populations_plot(\n",
    "    path=path, dirs=dirs,\n",
    "    tube=\"xx_79a_45_Ki67_3_xx.fcs\",\n",
    "    polygons=[\n",
    "        [\"Plasma cells\", \"orange\", (11, 7.5), (12, 10), (14, 7.5), (13, 5), (11, 2.6), (10, 6), (10, 7)],\n",
    "        [\"Lymphocytes\", \"teal\", (7, 4), (10, 4), (10, 1), (7.5, -1.5), (7, -1.5)],\n",
    "        [\"Lymphocytes\", \"teal\", (9.5, 6), (10, 6), (10, 5), (9, 4.5)],\n",
    "        [\"CD79a+\", \"yellow\", (9.5, 6), (10, 6), (10, 5), (9, 4.5)],\n",
    "        [\"CD3+\", \"red\", (7, 4), (10, 4), (10, 1), (7.5, -1.5), (7, -1.5)],\n",
    "        [\"Granulocytes\", \"yellow\", (14, 7.5), (18, 10), (18, 0), (7, -15), (4, -5), (7, -1.5), (7.5, -1.5), (10, 1), (13, 5)],\n",
    "        [\"Trash\", \"black\", (-6, 22), (18, 22), (11, 10), (9, 5), (7, 4), (5, 3), \n",
    "        (3, -12), (-6, -12)],\n",
    "        [\"Trash\", \"black\", (12, -10), (18, -10), (18, -15), (12, -15)]\n",
    "    ],\n",
    "    subsample=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5b4c20-12e6-4709-906f-5a76b99860dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in types:\n",
    "    for j in samples:\n",
    "        res = umap_populations_plot(\n",
    "            path=path, dirs=dirs,\n",
    "            tube=\"xx_79a_45_Ki67_3_xx.fcs\",\n",
    "            sample_type=i,\n",
    "            sample_name=j,\n",
    "            polygons=[\n",
    "                [\"Plasma cells\", \"orange\", (11, 7.5), (12, 10), (14, 7.5), (13, 5), (11, 2.6), (10, 6), (10, 7)],\n",
    "                [\"Lymphocytes\", \"teal\", (7, 4), (10, 4), (10, 1), (7.5, -1.5), (7, -1.5)],\n",
    "                [\"Lymphocytes\", \"teal\", (9.5, 6), (10, 6), (10, 5), (9, 4.5)],\n",
    "                [\"CD79a+\", \"yellow\", (9.5, 6), (10, 6), (10, 5), (9, 4.5)],\n",
    "                [\"CD3+\", \"red\", (7, 4), (10, 4), (10, 1), (7.5, -1.5), (7, -1.5)],\n",
    "                [\"Granulocytes\", \"yellow\", (14, 7.5), (18, 10), (18, 0), (7, -15), (4, -5), (7, -1.5), (7.5, -1.5), (10, 1), (13, 5)],\n",
    "                [\"Trash\", \"black\", (-6, 22), (18, 22), (11, 10), (9, 5), (7, 4), (5, 3), \n",
    "                (3, -12), (-6, -12)],\n",
    "                [\"Trash\", \"black\", (12, -10), (18, -10), (18, -15), (12, -15)]\n",
    "            ],\n",
    "            subsample=1\n",
    "        )\n",
    "\n",
    "        print(i)\n",
    "        print(j)\n",
    "        if res is not None and not res.empty:\n",
    "            print(res.describe().T.tail(9).iloc[:, :2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3165d678-3bf2-4a8f-8eaf-8a06096f3507",
   "metadata": {},
   "source": [
    "# Анализ тестовых файлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf422de-33b7-4ae7-a548-61244381a100",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "process_test_data(path, tube, pdf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4420313b-662b-4a87-889b-1b00c6edeea5",
   "metadata": {},
   "source": [
    "# Обращение к .csv-файлу отчёта и перекладка данных в бланк"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd996125-7e75-46ab-ab76-865a75c80111",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "make_report(path,  # Корневая папка, в которой лежит папка Reports с новыми отчётами.\n",
    "            blanks=r'C:\\Users\\vsevo\\Downloads',  # Папка с заключениями \n",
    "            coordinates=r'D:\\NovoExpress Data\\Examples\\Saved_UMAP_n_csv',  # Координаты для связи репорта и бланка заключения\n",
    "            clean=False)  # Если True (по умолчанию), отчётные файлы будут уладены"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688980e9-59f7-49ef-b2bb-6ee65c2bbf9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
